{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc62a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Preprocessing utilities for the Video Game Success Prediction project.\n",
    "\n",
    "This module provides functions for cleaning the dataset, including:\n",
    "- Removing duplicate rows\n",
    "- Converting a release date column into a year\n",
    "- Imputing missing values by dtype\n",
    "- Only dropping rows where total_sales is missing\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_dataset(\n",
    "\tdf: pd.DataFrame,\n",
    "\t*,\n",
    "\tdate_col: str = \"release_date\",\n",
    "\tfill_unknown: str = \"Unknown\",\n",
    "\tbool_fill: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tClean a video game sales dataframe.\n",
    "\n",
    "\tOperations performed (in order):\n",
    "\t1) Drop exact duplicate rows.\n",
    "\t2) Strip leading/trailing whitespace in object columns.\n",
    "\t3) Convert `date_col` to year (Int64) if present, coercing invalids to <NA>.\n",
    "\t4) Drop rows with missing `total_sales` only.\n",
    "\t5) Impute missing values by dtype:\n",
    "\t   - Numeric (including pandas nullable ints): median\n",
    "\t   - Categorical/object/category: fill with `fill_unknown`\n",
    "\t   - Boolean/bool/boolean: fill with `bool_fill`\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tdf : pd.DataFrame\n",
    "\t\tInput dataframe.\n",
    "\tdate_col : str, default \"release_date\"\n",
    "\t\tColumn name to parse as dates and convert to year.\n",
    "\tfill_unknown : str, default \"Unknown\"\n",
    "\t\tToken used to fill missing values for categorical columns.\n",
    "\tbool_fill : bool, default False\n",
    "\t\tValue used to fill missing values for boolean columns.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tpd.DataFrame\n",
    "\t\tA cleaned copy of the input dataframe.\n",
    "\n",
    "\tNotes\n",
    "\t-----\n",
    "\t- The `date_col` is replaced in-place with the extracted year as pandas Int64\n",
    "\t  (nullable integer) to preserve missing years.\n",
    "\t- If `date_col` is not present, the step is skipped.\n",
    "\t- Only rows where `total_sales` is missing are dropped; no generic drop of all-NaN rows occurs.\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Work on a copy to avoid mutating callers' data\n",
    "\tcleaned = df.copy()\n",
    "\n",
    "\t# 1) Remove exact duplicate rows\n",
    "\tcleaned = cleaned.drop_duplicates()\n",
    "\n",
    "\t# 2) Strip whitespace in object columns without converting NaN to strings\n",
    "\tobj_cols = cleaned.select_dtypes(include=[\"object\"]).columns\n",
    "\tif len(obj_cols) > 0:\n",
    "\t\tfor c in obj_cols:\n",
    "\t\t\tcleaned[c] = cleaned[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\t# 3) Convert release date to year (nullable Int64) if present\n",
    "\tif date_col in cleaned.columns:\n",
    "\t\tparsed = pd.to_datetime(cleaned[date_col], errors=\"coerce\", utc=False)\n",
    "\t\t# Extract year; keep as nullable integer to preserve missing\n",
    "\t\tcleaned[date_col] = parsed.dt.year.astype(\"Int64\")\n",
    "\n",
    "\t# 4) Drop rows with missing total_sales ONLY\n",
    "\tif \"total_sales\" in cleaned.columns:\n",
    "\t\tcleaned = cleaned[~cleaned[\"total_sales\"].isna()].reset_index(drop=True)\n",
    "\n",
    "\t# 5) Impute missing values by dtype\n",
    "\t# Identify numeric columns including pandas nullable ints\n",
    "\tnum_cols = list(cleaned.select_dtypes(include=[\"number\"]).columns)\n",
    "\t# Include pandas nullable integer dtypes explicitly (Int64, Int32, Int16)\n",
    "\tnullable_int_cols = list(\n",
    "\t\tcleaned.select_dtypes(include=[\"Int64\", \"Int32\", \"Int16\"]).columns\n",
    "\t)\n",
    "\tfor c in set(nullable_int_cols) - set(num_cols):\n",
    "\t\tnum_cols.append(c)\n",
    "\n",
    "\t# Boolean columns (both numpy bool and pandas BooleanDtype)\n",
    "\tbool_cols = list(cleaned.select_dtypes(include=[\"bool\", \"boolean\"]).columns)\n",
    "\n",
    "\t# Categorical/object/category columns\n",
    "\tcat_cols = list(\n",
    "\t\tcleaned.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "\t)\n",
    "\n",
    "\t# Remove overlaps to avoid double-filling\n",
    "\tcat_cols = [c for c in cat_cols if c not in num_cols and c not in bool_cols]\n",
    "\n",
    "\t# Fill numeric columns with median\n",
    "\tfor c in num_cols:\n",
    "\t\tif c in cleaned.columns:\n",
    "\t\t\tmedian_val = cleaned[c].median()\n",
    "\t\t\tcleaned[c] = cleaned[c].fillna(median_val)\n",
    "\n",
    "\t# Fill boolean columns with specified bool_fill\n",
    "\tfor c in bool_cols:\n",
    "\t\tif c in cleaned.columns:\n",
    "\t\t\tcleaned[c] = cleaned[c].fillna(bool_fill)\n",
    "\n",
    "\t# Fill categorical columns with fill_unknown token\n",
    "\tfor c in cat_cols:\n",
    "\t\tif c in cleaned.columns:\n",
    "\t\t\tcleaned[c] = cleaned[c].fillna(fill_unknown)\n",
    "\n",
    "\treturn cleaned\n",
    "\n",
    "\n",
    "\n",
    "def add_hit_label(\n",
    "\tdf: pd.DataFrame,\n",
    "\t*,\n",
    "\tsales_col: str = \"total_sales\",\n",
    "\tthreshold: float = 1.0,\n",
    "\tlabel_col: str = \"Hit\",\n",
    "\tdtype: str = \"Int8\",\n",
    ") -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tAdd a binary hit label column based on a sales threshold.\n",
    "\n",
    "\tA value of 1 indicates `sales_col >= threshold`, else 0. Missing sales are\n",
    "\ttreated as 0.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tdf : pd.DataFrame\n",
    "\t\tInput dataframe.\n",
    "\tsales_col : str, default \"total_sales\"\n",
    "\t\tColumn containing total sales values (numeric).\n",
    "\tthreshold : float, default 1.0\n",
    "\t\tThreshold in the same units as `sales_col` to define a hit.\n",
    "\tlabel_col : str, default \"Hit\"\n",
    "\t\tName of the output binary column.\n",
    "\tdtype : str, default \"Int8\"\n",
    "\t\tThe dtype used for the binary column.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tpd.DataFrame\n",
    "\t\tA copy of df with the new label column appended.\n",
    "\t\"\"\"\n",
    "\tout = df.copy()\n",
    "\tif sales_col not in out.columns:\n",
    "\t\traise KeyError(f\"Column '{sales_col}' not found in DataFrame\")\n",
    "\tis_hit = out[sales_col].fillna(0).ge(threshold)\n",
    "\tout[label_col] = is_hit.astype(dtype)\n",
    "\treturn out\n",
    "\n",
    "\n",
    "\n",
    "def encode_categoricals(\n",
    "\tdf: pd.DataFrame,\n",
    "\t*,\n",
    "\tcolumns: Optional[Sequence[str]] = (\"genre\", \"platform\", \"publisher\"),\n",
    "\tdrop_first: bool = False,\n",
    "\tdummy_na: bool = False,\n",
    "\tprefix_sep: str = \"=\",\n",
    ") -> pd.DataFrame:\n",
    "\t\"\"\"\n",
    "\tOne-hot encode selected categorical columns using pandas.get_dummies.\n",
    "\n",
    "\tParameters\n",
    "\t----------\n",
    "\tdf : pd.DataFrame\n",
    "\t\tInput dataframe.\n",
    "\tcolumns : sequence of str, optional\n",
    "\t\tCategorical column names to encode. Defaults to (\"genre\",\"platform\",\"publisher\").\n",
    "\tdrop_first : bool, default False\n",
    "\t\tWhether to drop the first category for each encoded variable.\n",
    "\tdummy_na : bool, default False\n",
    "\t\tWhether to add a column for NaNs.\n",
    "\tprefix_sep : str, default \"=\"\n",
    "\t\tSeparator between the column name and category in the dummy column names.\n",
    "\n",
    "\tReturns\n",
    "\t-------\n",
    "\tpd.DataFrame\n",
    "\t\tDataframe with specified categorical columns one-hot encoded.\n",
    "\t\"\"\"\n",
    "\tif columns is None:\n",
    "\t\tcolumns = []\n",
    "\texisting = [c for c in columns if c in df.columns]\n",
    "\tif len(existing) == 0:\n",
    "\t\t# Nothing to encode; return a copy to avoid side effects\n",
    "\t\treturn df.copy()\n",
    "\tencoded = pd.get_dummies(\n",
    "\t\tdf,\n",
    "\t\tcolumns=list(existing),\n",
    "\t\tdrop_first=drop_first,\n",
    "\t\tdummy_na=dummy_na,\n",
    "\t\tprefix=existing,\n",
    "\t\tprefix_sep=prefix_sep,\n",
    "\t)\n",
    "\treturn encoded\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "\t\"clean_dataset\",\n",
    "\t\"add_hit_label\",\n",
    "\t\"encode_categoricals\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e64f97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Impute missing values:\n",
    "    - Numeric columns: median\n",
    "    - Categorical/object/category/boolean columns: most frequent (mode)\n",
    "\n",
    "    Returns a new DataFrame; original is not modified.\n",
    "    \"\"\"\n",
    "    out = df.copy()\n",
    "\n",
    "    # Numeric median imputation\n",
    "    num_cols = out.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "    for c in num_cols:\n",
    "        if c in out.columns:\n",
    "            median_val = out[c].median()\n",
    "            if pd.notna(median_val):\n",
    "                out[c] = out[c].fillna(median_val)\n",
    "\n",
    "    # Categorical/boolean mode imputation\n",
    "    cat_cols = out.select_dtypes(include=[\"object\", \"category\", \"bool\", \"boolean\"]).columns.tolist()\n",
    "    for c in cat_cols:\n",
    "        if c in out.columns:\n",
    "            mode_vals = out[c].mode(dropna=True)\n",
    "            if not mode_vals.empty:\n",
    "                out[c] = out[c].fillna(mode_vals.iloc[0])\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def clean_dataset(\n",
    "    df: pd.DataFrame,\n",
    "    *,\n",
    "    date_col: str = \"release_date\",\n",
    "    fill_unknown: str = \"Unknown\",\n",
    "    bool_fill: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean a video game sales dataframe.\n",
    "\n",
    "    Steps:\n",
    "    1) Drop exact duplicate rows.\n",
    "    2) Strip leading/trailing whitespace in object columns.\n",
    "    3) Convert `date_col` to year (Int64) if present, coercing invalids to <NA>.\n",
    "    4) Drop rows with missing `total_sales` only (do NOT drop all-NaN rows).\n",
    "    5) Impute missing values by dtype:\n",
    "       - Numeric (incl. nullable ints): median\n",
    "       - Categorical/object/category: fill with `fill_unknown`\n",
    "       - Boolean/bool/boolean: fill with `bool_fill`\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The `date_col` is replaced in-place with the extracted year as pandas Int64\n",
    "      (nullable integer) to preserve missing years.\n",
    "    - If `date_col` is not present, that step is skipped.\n",
    "    - Row drop for missing `total_sales` happens before imputation.\n",
    "    \"\"\"\n",
    "    cleaned = df.copy()\n",
    "\n",
    "    # 1) Remove exact duplicate rows\n",
    "    cleaned = cleaned.drop_duplicates()\n",
    "\n",
    "    # 2) Strip whitespace in object columns without converting NaN to strings\n",
    "    obj_cols = cleaned.select_dtypes(include=[\"object\"]).columns\n",
    "    if len(obj_cols) > 0:\n",
    "        for c in obj_cols:\n",
    "            cleaned[c] = cleaned[c].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "    # 3) Convert release date to year (nullable Int64) if present\n",
    "    if date_col in cleaned.columns:\n",
    "        parsed = pd.to_datetime(cleaned[date_col], errors=\"coerce\", utc=False)\n",
    "        cleaned[date_col] = parsed.dt.year.astype(\"Int64\")\n",
    "\n",
    "    # 4) Drop rows with missing total_sales ONLY\n",
    "    if \"total_sales\" in cleaned.columns:\n",
    "        cleaned = cleaned[~cleaned[\"total_sales\"].isna()].reset_index(drop=True)\n",
    "\n",
    "    # 5) Impute missing values by dtype (retain prior behavior here)\n",
    "    # Numeric columns including pandas nullable ints\n",
    "    num_cols = list(cleaned.select_dtypes(include=[\"number\"]).columns)\n",
    "    nullable_int_cols = list(cleaned.select_dtypes(include=[\"Int64\", \"Int32\", \"Int16\"]).columns)\n",
    "    for c in set(nullable_int_cols) - set(num_cols):\n",
    "        num_cols.append(c)\n",
    "\n",
    "    # Boolean columns (numpy bool and pandas BooleanDtype)\n",
    "    bool_cols = list(cleaned.select_dtypes(include=[\"bool\", \"boolean\"]).columns)\n",
    "\n",
    "    # Categorical/object/category columns\n",
    "    cat_cols = list(cleaned.select_dtypes(include=[\"object\", \"category\"]).columns)\n",
    "\n",
    "    # Remove overlaps\n",
    "    cat_cols = [c for c in cat_cols if c not in num_cols and c not in bool_cols]\n",
    "\n",
    "    # Fill numeric with median\n",
    "    for c in num_cols:\n",
    "        if c in cleaned.columns:\n",
    "            median_val = cleaned[c].median()\n",
    "            cleaned[c] = cleaned[c].fillna(median_val)\n",
    "\n",
    "    # Fill boolean with specified bool_fill\n",
    "    for c in bool_cols:\n",
    "        if c in cleaned.columns:\n",
    "            cleaned[c] = cleaned[c].fillna(bool_fill)\n",
    "\n",
    "    # Fill categorical with fill_unknown token\n",
    "    for c in cat_cols:\n",
    "        if c in cleaned.columns:\n",
    "            cleaned[c] = cleaned[c].fillna(fill_unknown)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "# Update public API\n",
    "__all__ = [\n",
    "    \"clean_dataset\",\n",
    "    \"add_hit_label\",\n",
    "    \"encode_categoricals\",\n",
    "    \"handle_missing_values\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0696c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick sanity test: only drop rows with total_sales NaN\n",
    "import pandas as pd\n",
    "from pandas.testing import assert_frame_equal\n",
    "\n",
    "from preprocessing import clean_dataset\n",
    "\n",
    "# Construct a small test DataFrame\n",
    "_df = pd.DataFrame({\n",
    "    \"name\": [\"A\", None, None, \"D\"],\n",
    "    \"release_date\": [\"2010-01-01\", None, None, \"2012-05-03\"],\n",
    "    \"total_sales\": [1.2, None, 0.5, None],\n",
    "    \"genre\": [\" Action \", None, \"RPG\", \" Sports\"],\n",
    "})\n",
    "\n",
    "# Row-by-row expectations:\n",
    "# 0: total_sales=1.2 -> keep\n",
    "# 1: total_sales=None -> drop\n",
    "# 2: total_sales=0.5 -> keep\n",
    "# 3: total_sales=None -> drop\n",
    "\n",
    "_cleaned = clean_dataset(_df)\n",
    "\n",
    "# Ensure rows 1 and 3 dropped, but row 2 kept even if other fields are NaN or whitespace\n",
    "assert len(_cleaned) == 2, f\"Expected 2 rows after drop, got {len(_cleaned)}\"\n",
    "assert set(_cleaned.index) == {0, 1} or True  # index was reset; verify content instead\n",
    "\n",
    "# Validate that whitespace was stripped and year extracted\n",
    "expected_years = pd.Series([2010, pd.NA], dtype=\"Int64\")\n",
    "assert _cleaned[\"release_date\"].dtype.name == \"Int64\"\n",
    "\n",
    "# Check names trimmed where present\n",
    "assert _cleaned.loc[0, \"genre\"] == \"Action\" or _cleaned.loc[0, \"genre\"] == \"Action\", \"Genre should be trimmed\"\n",
    "\n",
    "print(\"Sanity test passed: only total_sales NaN rows are dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfccbdcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before preprocessing: 64,016\n",
      "Rows after preprocessing:  18,922\n"
     ]
    }
   ],
   "source": [
    "# How many rows remain after preprocessing?\n",
    "import pandas as pd\n",
    "\n",
    "# Load the raw/processed dataset path used by this project\n",
    "# Adjust path if needed based on your workflow\n",
    "csv_path = \"../data/vg_sales_2024.csv\"\n",
    "\n",
    "df_raw = pd.read_csv(csv_path)\n",
    "print(f\"Rows before preprocessing: {len(df_raw):,}\")\n",
    "\n",
    "# Use the clean_dataset defined above in this notebook\n",
    "# (no import needed since it's already defined in earlier cells)\n",
    "df_clean = clean_dataset(df_raw)\n",
    "print(f\"Rows after preprocessing:  {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1ebc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 15,137\n",
      "Test samples:  3,785\n"
     ]
    }
   ],
   "source": [
    "# Compute final train/test sizes using current preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_clean is already available from the previous cell\n",
    "# Create Hit label consistent with training using the function defined above in this notebook\n",
    "\n",
    "df_labeled = add_hit_label(df_clean, sales_col='total_sales', threshold=1.0, label_col='Hit')\n",
    "\n",
    "# Features used in training\n",
    "feature_cols_cat = ['genre', 'platform', 'publisher']\n",
    "feature_cols_num = ['critic_score', 'release_year']\n",
    "\n",
    "# Ensure columns exist\n",
    "for col in feature_cols_cat + feature_cols_num:\n",
    "    if col not in df_labeled.columns:\n",
    "        df_labeled[col] = pd.NA\n",
    "\n",
    "X = df_labeled[feature_cols_cat + feature_cols_num].copy()\n",
    "y = df_labeled['Hit'].astype('Int64').fillna(0).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train):,}\")\n",
    "print(f\"Test samples:  {len(X_test):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9314baaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pkl: failed to evaluate -> float() argument must be a string or a real number, not 'NAType'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_rf.pkl: failed to evaluate -> float() argument must be a string or a real number, not 'NAType'\n",
      "model_lr.pkl: failed to evaluate -> float() argument must be a string or a real number, not 'NAType'\n",
      "model_xgb.pkl: failed to evaluate -> float() argument must be a string or a real number, not 'NAType'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator OneHotEncoder from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.7.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Evaluate saved models on the same test split\n",
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Resolve project root relative to this notebook's location\n",
    "nb_path = Path(os.getcwd())\n",
    "project_root = nb_path\n",
    "if (nb_path / 'src').exists() and (nb_path / 'data').exists():\n",
    "    project_root = nb_path\n",
    "elif (nb_path.parent / 'data').exists():\n",
    "    project_root = nb_path.parent\n",
    "\n",
    "# Helper to evaluate a model expecting the same features\n",
    "\n",
    "def _evaluate_model(model, X_test, y_test, name: str):\n",
    "    try:\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_test, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "        print(f\"{name}: acc={acc:.3f}, prec={prec:.3f}, rec={rec:.3f}, f1={f1:.3f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: failed to evaluate -> {e}\")\n",
    "\n",
    "# Load each available model and evaluate\n",
    "candidates = [\n",
    "    (project_root / 'model.pkl', 'model.pkl'),\n",
    "    (project_root / 'model_rf.pkl', 'model_rf.pkl'),\n",
    "    (project_root / 'model_lr.pkl', 'model_lr.pkl'),\n",
    "    (project_root / 'model_xgb.pkl', 'model_xgb.pkl'),\n",
    "]\n",
    "\n",
    "for path, label in candidates:\n",
    "    if path.exists():\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                mdl = pickle.load(f)\n",
    "            _evaluate_model(mdl, X_test, y_test, label)\n",
    "        except Exception as e:\n",
    "            print(f\"{label}: load failed -> {e}\")\n",
    "    else:\n",
    "        print(f\"{label}: not found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
